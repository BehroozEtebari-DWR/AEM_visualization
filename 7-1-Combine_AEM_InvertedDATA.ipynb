{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cf0be0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pyproj import CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b7cf8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\betebari\\AppData\\Local\\Temp\\ipykernel_14668\\2562755918.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\betebari\\AppData\\Local\\Temp\\ipykernel_14668\\2562755918.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\betebari\\AppData\\Local\\Temp\\ipykernel_14668\\2562755918.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\betebari\\AppData\\Local\\Temp\\ipykernel_14668\\2562755918.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LINE_NO         UTMX          UTMY     TIMESTAMP  FID  RECORD  ELEVATION  \\\n",
      "0   100101  6946.546875 -278186.81250  44541.886668    1       1      388.4   \n",
      "1   100101  6932.914062 -278152.59375  44541.886684    2       2      387.8   \n",
      "2   100101  6922.117188 -278117.78125  44541.886700    3       3      387.5   \n",
      "3   100101  6913.581055 -278082.68750  44541.886716    4       4      387.5   \n",
      "4   100101  6906.829102 -278047.43750  44541.886733    5       5      387.6   \n",
      "\n",
      "     ALT  INVALT  INVALTSTD  ...  DEP_BOT_22  DEP_BOT_23  DEP_BOT_24  \\\n",
      "0  30.39   30.26     0.0317  ...     234.241     268.541     307.581   \n",
      "1  29.57   29.37     0.0329  ...     234.241     268.541     307.581   \n",
      "2  28.87   28.79     0.0324  ...     234.241     268.541     307.581   \n",
      "3  28.92   28.87     0.0328  ...     234.241     268.541     307.581   \n",
      "4  29.12   29.27     0.0328  ...     234.241     268.541     307.581   \n",
      "\n",
      "   DEP_BOT_25  DEP_BOT_26  DEP_BOT_27  DEP_BOT_28  DEP_BOT_29  \\\n",
      "0     352.001     402.541     460.061     525.511     599.981   \n",
      "1     352.001     402.541     460.061     525.511     599.981   \n",
      "2     352.001     402.541     460.061     525.511     599.981   \n",
      "3     352.001     402.541     460.061     525.511     599.981   \n",
      "4     352.001     402.541     460.061     525.511     599.981   \n",
      "\n",
      "   DOI_CONSERVATIVE  DOI_STANDARD  \n",
      "0           151.592       216.263  \n",
      "1           159.517       225.540  \n",
      "2           174.274       244.602  \n",
      "3           187.307       263.134  \n",
      "4           199.769       280.706  \n",
      "\n",
      "[5 rows x 112 columns]\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the Inverted XYZ files\n",
    "directory_path = r'C:\\Users\\betebari\\Documents\\C2VSim_Texture\\AEM supporting data\\RAW Data'\n",
    "\n",
    "# Get a list of all .xyz files in the directory\n",
    "file_paths = glob.glob(os.path.join(directory_path, '*.xyz'))\n",
    "\n",
    "# Initialize an empty list to collect DataFrames\n",
    "dataframes = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    try:\n",
    "        # Read the first 27 rows to capture the header and inspect it\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        \n",
    "        # Clean the header (27th line) by removing or replacing \"/\"\n",
    "        header = lines[26].replace(\"/\", \"\").strip()\n",
    "        \n",
    "        # Write a cleaned version of the file to a temporary file for processing\n",
    "        temp_file_path = file_path + \".temp\"\n",
    "        with open(temp_file_path, 'w') as temp_file:\n",
    "            temp_file.write(header + \"\\n\")  # Write the cleaned header\n",
    "            temp_file.writelines(lines[27:])  # Write the rest of the data\n",
    "        \n",
    "        # Read the cleaned temporary file into a DataFrame\n",
    "        df = pd.read_csv(\n",
    "            temp_file_path,\n",
    "            delim_whitespace=True,  # Use whitespace as a delimiter\n",
    "            header=0,               # Use the cleaned header\n",
    "            encoding='latin1',      # Handle special characters\n",
    "            low_memory=False        # Avoid dtype warnings\n",
    "        )\n",
    "        \n",
    "        # Drop all columns with names starting with specified prefixes\n",
    "        prefixes_to_drop = ('SIGMA_I_', 'RHO_I_STD', 'THK_', 'THK_STD_', 'DEP_TOP_STD_', 'DEP_BOT_STD_')\n",
    "        df = df.loc[:, ~df.columns.str.startswith(prefixes_to_drop)]\n",
    "        \n",
    "        # Check for empty or invalid DataFrames\n",
    "        if df.empty:\n",
    "            print(f\"File {file_path} resulted in an empty DataFrame. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dataframes.append(df)\n",
    "        \n",
    "        # Remove the temporary file\n",
    "        os.remove(temp_file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading {file_path}: {e}\")\n",
    "\n",
    "# Combine all DataFrames into one, if any were successfully read\n",
    "if dataframes:\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    print(combined_df.head())  # Preview the combined DataFrame\n",
    "    # Save to CSV (optional)\n",
    "    # combined_df.to_csv(r'C:\\path_to_save\\combined_data.csv', index=False)\n",
    "else:\n",
    "    print(\"No valid dataframes were created. Please check the files and script.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb707d6-83de-4dc3-a8ed-1d033d62071c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTMX range: -368168.3125 to 117673.421875\n",
      "UTMY range: -344393.875 to 305633.21875\n",
      "Transformed UTMX_26910 range: 385815.3533092168 to 890811.0948630879\n",
      "Transformed UTMY_26910 range: 3870833.311288326 to 4505524.361896835\n",
      "Transformation complete. Transformed coordinates added as new columns.\n"
     ]
    }
   ],
   "source": [
    "# Load your data (ensure `combined_df` exists)\n",
    "df = combined_df.copy()  # Replace with the correct DataFrame if already using `df`\n",
    "\n",
    "# Define CRS for input (EPSG: 3310) and output (EPSG: 26910)\n",
    "input_crs = CRS.from_epsg(3310)\n",
    "output_crs = CRS.from_epsg(26910)\n",
    "\n",
    "# Step 1: Check for missing or invalid coordinates\n",
    "if 'UTMX' not in df.columns or 'UTMY' not in df.columns:\n",
    "    raise ValueError(\"The required 'UTMX' and 'UTMY' columns are missing.\")\n",
    "if df['UTMX'].isna().any() or df['UTMY'].isna().any():\n",
    "    raise ValueError(\"NaN values found in 'UTMX' or 'UTMY' columns.\")\n",
    "\n",
    "# Step 2: Ensure the coordinates make sense for EPSG: 3310\n",
    "# (e.g., X and Y should be in meters for projected CRS)\n",
    "print(f\"UTMX range: {df['UTMX'].min()} to {df['UTMX'].max()}\")\n",
    "print(f\"UTMY range: {df['UTMY'].min()} to {df['UTMY'].max()}\")\n",
    "\n",
    "# Step 3: Create a GeoDataFrame with the input CRS\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df,\n",
    "    geometry=gpd.points_from_xy(df['UTMX'], df['UTMY']),\n",
    "    crs=input_crs\n",
    ")\n",
    "\n",
    "# Step 4: Transform to the desired CRS (EPSG: 26910)\n",
    "try:\n",
    "    gdf = gdf.to_crs(output_crs)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error during CRS transformation: {e}\")\n",
    "\n",
    "# Step 5: Extract transformed coordinates\n",
    "df['UTMX_26910'] = gdf.geometry.x\n",
    "df['UTMY_26910'] = gdf.geometry.y\n",
    "\n",
    "# Step 6: Check the ranges of transformed coordinates\n",
    "print(f\"Transformed UTMX_26910 range: {df['UTMX_26910'].min()} to {df['UTMX_26910'].max()}\")\n",
    "print(f\"Transformed UTMY_26910 range: {df['UTMY_26910'].min()} to {df['UTMY_26910'].max()}\")\n",
    "\n",
    "print(\"Transformation complete. Transformed coordinates added as new columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b2e28c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Conversion meters to feet\n",
    "df['ELEVATION_ft'] = df['ELEVATION'] * 3.28084\n",
    "\n",
    "# Define the conversion factor (1 meter = 3.28084 feet)\n",
    "meters_to_feet = 3.28084\n",
    "\n",
    "# Identify columns starting with 'DEP_TOP_' or 'DEP_BOT_'\n",
    "columns_to_convert = [col for col in df.columns if col.startswith('DEP_TOP_') or col.startswith('DEP_BOT_')]\n",
    "\n",
    "# Create new columns with feet values and drop the original meter columns\n",
    "for col in columns_to_convert:\n",
    "    new_col_name = f\"{col}_feet\"\n",
    "    df[new_col_name] = df[col] * meters_to_feet\n",
    "\n",
    "# Drop the original meter columns\n",
    "df.drop(columns=columns_to_convert, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99868124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop specified columns\n",
    "df = df.drop(columns=['ELEVATION','UTMX','UTMY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52bd2f1a-d7f5-4db0-9832-c845655a284c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LINE_NO     UTMX_26910    UTMY_26910  ELEVATION_ft  Layer  DEP_TOP_feet  \\\n",
      "0   100101  779030.095494  3.934318e+06   1274.278256      1           0.0   \n",
      "1   100101  779015.406499  3.934351e+06   1272.309752      1           0.0   \n",
      "2   100101  779003.538843  3.934386e+06   1271.325500      1           0.0   \n",
      "3   100101  778993.926125  3.934421e+06   1271.325500      1           0.0   \n",
      "4   100101  778986.095001  3.934456e+06   1271.653584      1           0.0   \n",
      "\n",
      "   DEP_BOT_feet RHO_Layer  RHO_Value  Altitude_TOP_ft  Altitude_BOT_ft  \n",
      "0       6.56168   RHO_I_1      27.81      1274.278256      1267.716576  \n",
      "1       6.56168   RHO_I_1      26.57      1272.309752      1265.748072  \n",
      "2       6.56168   RHO_I_1      28.42      1271.325500      1264.763820  \n",
      "3       6.56168   RHO_I_1      32.16      1271.325500      1264.763820  \n",
      "4       6.56168   RHO_I_1      37.78      1271.653584      1265.091904  \n"
     ]
    }
   ],
   "source": [
    "# Identify columns to melt\n",
    "dep_top_cols = [col for col in df.columns if col.startswith('DEP_TOP_')]\n",
    "dep_bot_cols = [col for col in df.columns if col.startswith('DEP_BOT_')]\n",
    "rho_cols = [col for col in df.columns if col.startswith('RHO_I_')]\n",
    "\n",
    "# Melt DEP_TOP and DEP_BOT columns into rows\n",
    "top_melted = df.melt(\n",
    "    id_vars=['LINE_NO', 'UTMX_26910', 'UTMY_26910', 'ELEVATION_ft'],  # Keep these columns intact\n",
    "    value_vars=dep_top_cols,\n",
    "    var_name='Layer',\n",
    "    value_name='DEP_TOP_feet'\n",
    ")\n",
    "\n",
    "bot_melted = df.melt(\n",
    "    id_vars=['LINE_NO', 'UTMX_26910', 'UTMY_26910', 'ELEVATION_ft'],\n",
    "    value_vars=dep_bot_cols,\n",
    "    var_name='Layer',\n",
    "    value_name='DEP_BOT_feet'\n",
    ")\n",
    "\n",
    "# Ensure 'Layer' names match by extracting the numeric suffix\n",
    "top_melted['Layer'] = top_melted['Layer'].str.extract(r'(\\d+)', expand=False).astype(float)\n",
    "bot_melted['Layer'] = bot_melted['Layer'].str.extract(r'(\\d+)', expand=False).astype(float)\n",
    "\n",
    "# Drop rows with NaN in 'Layer' (if any)\n",
    "top_melted = top_melted.dropna(subset=['Layer'])\n",
    "bot_melted = bot_melted.dropna(subset=['Layer'])\n",
    "\n",
    "# Convert 'Layer' to integer\n",
    "top_melted['Layer'] = top_melted['Layer'].astype(int)\n",
    "bot_melted['Layer'] = bot_melted['Layer'].astype(int)\n",
    "\n",
    "# Merge melted DataFrames\n",
    "reshaped_df = top_melted.merge(\n",
    "    bot_melted,\n",
    "    on=['LINE_NO', 'UTMX_26910', 'UTMY_26910', 'ELEVATION_ft', 'Layer']\n",
    ")\n",
    "\n",
    "# Add RHO_I_* data\n",
    "rho_melted = df.melt(\n",
    "    id_vars=['LINE_NO', 'UTMX_26910', 'UTMY_26910', 'ELEVATION_ft'],\n",
    "    value_vars=rho_cols,\n",
    "    var_name='RHO_Layer',\n",
    "    value_name='RHO_Value'\n",
    ")\n",
    "\n",
    "# Extract the layer number from RHO_Layer and align it with reshaped_df\n",
    "rho_melted['Layer'] = rho_melted['RHO_Layer'].str.extract(r'(\\d+)', expand=False).astype(float).astype(int)\n",
    "\n",
    "# Merge with reshaped DataFrame, aligning by Layer\n",
    "reshaped_df = reshaped_df.merge(\n",
    "    rho_melted,\n",
    "    on=['LINE_NO', 'UTMX_26910', 'UTMY_26910', 'ELEVATION_ft', 'Layer']\n",
    ")\n",
    "\n",
    "# Calculate altitudes from the depth values\n",
    "reshaped_df['Altitude_TOP_ft'] = reshaped_df['ELEVATION_ft'] - reshaped_df['DEP_TOP_feet']\n",
    "reshaped_df['Altitude_BOT_ft'] = reshaped_df['ELEVATION_ft'] - reshaped_df['DEP_BOT_feet']\n",
    "\n",
    "# Display a preview of the reshaped DataFrame\n",
    "print(reshaped_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea65c52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted data saved to: Updated_AEM_INVERTED.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the updated DataFrame to a new CSV\n",
    "\n",
    "# Step 1: Drop unnecessary columns\n",
    "columns_to_drop = [\n",
    "    'DEP_TOP_feet', 'DEP_BOT_feet', 'RHO_Layer',\n",
    "]\n",
    "\n",
    "output_csv = r\"Updated_AEM_INVERTED.csv\"\n",
    "reshaped_df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Converted data saved to: {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a7a160-e50f-4d41-a5d1-13febda7d587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
